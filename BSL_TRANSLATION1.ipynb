{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36363bcf",
   "metadata": {},
   "source": [
    "### Import and install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd401d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "#!pip install tensorflow==2.13.0rc1 opencv-python sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05225f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install mediapipe\n",
    "#!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import openCV\n",
    "import cv2 \n",
    "\n",
    "#import numPy\n",
    "import numpy as np \n",
    "\n",
    "#import os\n",
    "import os \n",
    "\n",
    "#import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#import time\n",
    "import time \n",
    "\n",
    "#import mediaPipe\n",
    "import mediapipe as mp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293c7a4",
   "metadata": {},
   "source": [
    "# 1. Keypoint Extraction using MediaPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebfb88f",
   "metadata": {},
   "source": [
    "##### Creating variables and functions for keypoint Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b546f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the variables and assigning functions\n",
    "mediapipe_holistic = mp.solutions.holistic \n",
    "mediapipe_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93dc840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating detection function\n",
    "def detection_function(image, model):\n",
    "    #convert BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    #make image non-writeable\n",
    "    image.flags.writeable = False \n",
    "    #make prediction\n",
    "    detected_landmarks = model.process(image) \n",
    "    #make image writeable\n",
    "    image.flags.writeable = True \n",
    "    #convert RGB 2 BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, detected_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93725fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize the landmarks using 'mediapipe_drawing' variable\n",
    "def draw_styled_landmarks(image, detected_landmarks):\n",
    "    # Draw left hand connections\n",
    "    mediapipe_drawing.draw_landmarks(image, detected_landmarks.left_hand_landmarks, mediapipe_holistic.HAND_CONNECTIONS, \n",
    "                             mediapipe_drawing.DrawingSpec(color=(1,255,255), thickness=2, circle_radius=4), \n",
    "                             mediapipe_drawing.DrawingSpec(color=(255,15,10), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mediapipe_drawing.draw_landmarks(image, detected_landmarks.right_hand_landmarks, mediapipe_holistic.HAND_CONNECTIONS, \n",
    "                             mediapipe_drawing.DrawingSpec(color=(5,255,3), thickness=2, circle_radius=4), \n",
    "                             mediapipe_drawing.DrawingSpec(color=(9,9,255), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1980fd2f",
   "metadata": {},
   "source": [
    "##### Extracting keypoints values from captured video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access webcam (video capture device (0))\n",
    "cam = cv2.VideoCapture(0) \n",
    "\n",
    "\n",
    "# Set mediapipe model\n",
    "with mediapipe_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "# begin while loop\n",
    "    while cam.isOpened():\n",
    "\n",
    "            # Read feed\n",
    "            return_value, image_frame = cam.read()\n",
    "\n",
    "            # Make detections\n",
    "            # get the 'image' and 'detected_landmarks' \n",
    "            image, detected_landmarks = detection_function(image_frame, holistic)\n",
    "            print(detected_landmarks)\n",
    "\n",
    "            # Draw the landmarks\n",
    "            draw_styled_landmarks(image, detected_landmarks)\n",
    "    \n",
    "            # Show o screen\n",
    "            cv2.imshow('OpenCV window', image)\n",
    "\n",
    "            # break statement\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "            #while loop end\n",
    "            \n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80373b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing the last frame to display landmark values (left-hand)\n",
    "detected_landmarks.left_hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of the detected landmarks (left hand)\n",
    "len(detected_landmarks.left_hand_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fff3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest video frame inform of array\n",
    "image_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ee8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying draw_styled_landmarks to current frame\n",
    "draw_styled_landmarks(image_frame, detected_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84918794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visuaize the current captured frame in RGB format using matplotlib\n",
    "plt.imshow(cv2.cvtColor(image_frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35d17d",
   "metadata": {},
   "source": [
    "##### Store extracted Keypoints into numPy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating variables to store extracted keypoints in a flattened array\n",
    "left_hand = np.array([[res.x, res.y, res.z] for res in detected_landmarks.left_hand_landmarks.landmark]).flatten() if detected_landmarks.left_hand_landmarks else np.zeros(21*3) \n",
    "right_hand = np.array([[res.x, res.y, res.z] for res in detected_landmarks.right_hand_landmarks.landmark]).flatten() if detected_landmarks.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying keypoint values for left hand\n",
    "left_hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c55b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of left_hand array # 21*3 = 63\n",
    "left_hand.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf5a7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keypoint values for right hand \n",
    "right_hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of right_hand array shape #np.zeros(21*3) = 63\n",
    "right_hand.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301795c5",
   "metadata": {},
   "source": [
    "###### Function to extract keypoints and concatenate into a single array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bdd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract keypoints and concatenate into single array \n",
    "def mediapipe_keypoints(detected_landmarks):\n",
    "    left_hand = np.array([[res.x, res.y, res.z] for res in detected_landmarks.left_hand_landmarks.landmark]).flatten() if detected_landmarks.left_hand_landmarks else np.zeros(21*3)\n",
    "    right_hand = np.array([[res.x, res.y, res.z] for res in detected_landmarks.right_hand_landmarks.landmark]).flatten() if detected_landmarks.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([ left_hand, right_hand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the final shape of the concatenated array\n",
    "mediapipe_keypoints(detected_landmarks).shape \n",
    "\n",
    "#expected result: \n",
    "# (left-hand keypoints * (x,y,z co-ordinates)) + (right-hand keypoints * (x,y,z co-ordinates))\n",
    "# 21*3 + 21*3 = 126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edcc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the resultant array in a variable\n",
    "total_keypoints = mediapipe_keypoints(detected_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03263b81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Displaing the concatenated array\n",
    "total_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16882a79",
   "metadata": {},
   "source": [
    "# 2. Create Datasets for BSL fingerspelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae736e",
   "metadata": {},
   "source": [
    "##### Set variables for Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# BSL fingerspelling alphabets\n",
    "alphabets = np.array(['A', 'B', 'C', 'D', 'E', 'F',\n",
    "                    'G', 'H', 'I', 'J', 'K', 'L', \n",
    "                    'M', 'N', 'O', 'P', 'Q', 'R', \n",
    "                    'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])\n",
    "\n",
    "# 30 video seqences per each alphapbet \n",
    "no_sequences = 30\n",
    "\n",
    "# 20 frames per each video sequence \n",
    "sequence_length = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b8644d",
   "metadata": {},
   "source": [
    "##### Create Folders for Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b1fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders\n",
    "for alphabet in alphabets: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, alphabet, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896acc6d",
   "metadata": {},
   "source": [
    "##### Collecting datasets using openCV and mediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236856e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access webcam (video capture device (1)\n",
    "cam = cv2.VideoCapture(1)\n",
    "\n",
    "# Set mediapipe model \n",
    "with mediapipe_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # Loop through each alphabet\n",
    "    for alphabet in alphabets:\n",
    "        # Loop through each video sequence\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through sequence length of each video\n",
    "            for frame_number in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                return_value, image_frame = cam.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, detected_landmarks = detection_function(image_frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, detected_landmarks)\n",
    "                \n",
    "                # creating of datasets\n",
    "                if frame_number == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(alphabet, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.imshow('OpenCV Data Collection', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(alphabet, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Data Collection', image)\n",
    "                \n",
    "                # Export keypoints\n",
    "                keypoints = mediapipe_keypoints(detected_landmarks)\n",
    "                npy_path = os.path.join(DATA_PATH, alphabet, str(sequence), str(frame_number))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break loop\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7312d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b20534",
   "metadata": {},
   "source": [
    "##### labeling datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a lable dictionary to represent the alphabet index and their labels\n",
    "alphabet_labels = {label:num for num, label in enumerate(alphabets)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display labels\n",
    "alphabet_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29cde6",
   "metadata": {},
   "source": [
    "##### Combine all data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing all data together and structuring it into a single array\n",
    "\n",
    "# initializing empty arrays\n",
    "sequences, labels = [], []\n",
    "for alphabet in alphabets:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_number in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, alphabet, str(sequence), \"{}.npy\".format(frame_number)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(alphabet_labels[alphabet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shape of final array\n",
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a898771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shape of the labels\n",
    "np.array(labels).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f09bd0",
   "metadata": {},
   "source": [
    "##### preprocess data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab26c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies for splitting dataset and convert data using one-hot encoding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the sequences in 'X'\n",
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shape of 'X'\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff882208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the labels into binary flat using one-hot encoding\n",
    "Y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9f7398",
   "metadata": {},
   "source": [
    "##### Split dataset into Train and Test categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into training and testing (training data = 90%, testing data = 10%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf77579",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99caa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shapes of training and testing data after the splitting of datasets\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080afb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364fdc6",
   "metadata": {},
   "source": [
    "# 3. Training dataset using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf8f68",
   "metadata": {},
   "source": [
    "##### Build LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import legacy as keras_legacy_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6032d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing TensorBoard logs\n",
    "log_dir = os.path.join('Logs') \n",
    "\n",
    "#integrating TensorBoard with the model training process.\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing an empty LSTM Neural Network model\n",
    "model = Sequential()\n",
    "\n",
    "# adding LSTM layer to model which as 64 units, and uses 'tanh' activation function\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(20, 126)))\n",
    "# adding another LSTM layer with 128 units and'tanh' activation function\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "# adding LSTM layer to model which as 64 units and'tanh' activation function\n",
    "model.add(LSTM(64, return_sequences=False, activation='tanh'))\n",
    "\n",
    "# adding a dense layer with 64 units \n",
    "model.add(Dense(64, activation='tanh'))\n",
    "# adding a dense layer with 32 units\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "# adding a dense layer with units equal to number of categories(alphabets) and 'softmax' activation function\n",
    "model.add(Dense(alphabets.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the number of outputs in the final layer\n",
    "alphabets.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99032c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [0.7, 0.2, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfca82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring the training process of the model\n",
    "model.compile(optimizer=keras_legacy_optimizer.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac51aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the summary of the LSTM model built\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe2363",
   "metadata": {},
   "source": [
    "##### Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c59527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model. \n",
    "model.fit(X_train, Y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac220cae",
   "metadata": {},
   "source": [
    "##### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c69da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved model\n",
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8a9722",
   "metadata": {},
   "source": [
    "# 4. Evaluation and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88017e11",
   "metadata": {},
   "source": [
    "##### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ff110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing results inside 'res' variable\n",
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b8bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81724bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets[np.argmax(Y_test[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets[np.argmax(res[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592458c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets[np.argmax(Y_test[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbc346",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets[np.argmax(res[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fd4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets[np.argmax(Y_test[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc795fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets[np.argmax(res[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346af29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets[np.argmax(Y_test[3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6344aa",
   "metadata": {},
   "source": [
    "##### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions on test data using trained LSTM model\n",
    "ypredict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d590c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting true labels from 'Y_test'\n",
    "ytrue = np.argmax(Y_test, axis=1).tolist()\n",
    "\n",
    "# extracting predicted labels from 'yhat'\n",
    "ypredict = np.argmax(ypredict, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7978bae",
   "metadata": {},
   "source": [
    "##### Evaluate using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01575a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2257c91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculating multilabel confusion matrix\n",
    "multilabel_confusion_matrix(ytrue, ypredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1708a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing confusion matrix in variable\n",
    "multilabel_cm = multilabel_confusion_matrix(ytrue, ypredict)\n",
    "\n",
    "# Define the class labels\n",
    "class_labels = ['A', 'B', 'C', 'D', 'E', 'F',\n",
    "                    'G', 'H', 'I', 'J', 'K', 'L', \n",
    "                    'M', 'N', 'O', 'P', 'Q', 'R', \n",
    "                    'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']  \n",
    "\n",
    "# Create a function to plot a confusion matrix heatmap\n",
    "def plot_confusion_matrix(conf_matrix, class_labels):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, cm in enumerate(conf_matrix):\n",
    "        plt.subplot(5, 6, i + 1) \n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "        plt.title(f\"Class {class_labels[i]}\")\n",
    "        plt.xlabel(\"True\")\n",
    "        plt.ylabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the confusion matrix heatmap\n",
    "plot_confusion_matrix(multilabel_cm, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1ee62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(ytrue, ypredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b46f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable storing confusion matrix\n",
    "cm = confusion_matrix(ytrue, ypredict)\n",
    "\n",
    "# Visualize the confusion matrix using a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "        xticklabels=['A', 'B', 'C', 'D', 'E', 'F',\n",
    "                    'G', 'H', 'I', 'J', 'K', 'L', \n",
    "                    'M', 'N', 'O', 'P', 'Q', 'R', \n",
    "                    'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], \n",
    "        yticklabels=['A', 'B', 'C', 'D', 'E', 'F',\n",
    "                    'G', 'H', 'I', 'J', 'K', 'L', \n",
    "                    'M', 'N', 'O', 'P', 'Q', 'R', \n",
    "                    'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])\n",
    "plt.xlabel('True Label')\n",
    "plt.ylabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2adb2e",
   "metadata": {},
   "source": [
    "##### Evauate using evaluation metrics (accuracy, f1-score, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the accuracy, F1, precision, recall for all classes\n",
    "accuracy = accuracy_score(ytrue, ypredict) \n",
    "f1 = f1_score(ytrue, ypredict, average='macro')\n",
    "precision = precision_score(ytrue, ypredict, average='macro')\n",
    "recall = recall_score(ytrue, ypredict, average='macro')\n",
    "\n",
    "print(f\"accuracy: {accuracy}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(f\"precision-score: {precision}\")\n",
    "print(f\"recall-score: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab757f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1-Score', 'Precision', 'Recall'],\n",
    "    'Score': [accuracy, f1, precision, recall]\n",
    "})\n",
    "\n",
    "# Plotting the metrics using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Metric', y='Score', data=metrics_df, palette='viridis')\n",
    "plt.title('Overall Performance Metrics')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba753d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating precision score for each class (alphabet)\n",
    "precision_scores = precision_score(ytrue, ypredict, average=None)\n",
    "\n",
    "# Printing precision scores for each class\n",
    "for idx, alphabet in enumerate(alphabets):\n",
    "    print(f\"Precision for Alphabet {alphabet}: {precision_scores[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a710fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(alphabets, precision_scores)\n",
    "plt.xlabel('Alphabets')\n",
    "plt.ylabel('Precision Score')\n",
    "plt.title('Precision Scores for Each Alphabet')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "alphabet_accuracy = {}\n",
    "\n",
    "for idx, alphabet in enumerate(alphabets):\n",
    "    indices = [i for i, y in enumerate(ytrue) if y == idx]\n",
    "    class_ytrue = [ytrue[i] for i in indices]\n",
    "    class_ypredict = [ypredict[i] for i in indices]\n",
    "    accuracy = accuracy_score(class_ytrue, class_ypredict)\n",
    "    alphabet_accuracy[alphabet] = accuracy\n",
    "\n",
    "# Printing accuracy for each class\n",
    "for alphabet, accuracy in alphabet_accuracy.items():\n",
    "    print(f\"Accuracy for Alphabet {alphabet}: {accuracy:.4f}\")\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(alphabet_accuracy.keys(), alphabet_accuracy.values())\n",
    "plt.xlabel('Alphabets')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Accuracy Scores for Each Alphabet')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating F1 score for each class (alphabet)\n",
    "f1_scores = f1_score(ytrue, ypredict, average=None)\n",
    "\n",
    "# Printing F1 scores for each class\n",
    "for idx, alphabet in enumerate(alphabets):\n",
    "    print(f\"F1 Score for Alphabet {alphabet}: {f1_scores[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab80333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(alphabets, f1_scores)\n",
    "plt.xlabel('Alphabets')\n",
    "plt.ylabel('F1-scores')\n",
    "plt.title('F1-score for Each Alphabet')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44efa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating recall score for each class (alphabet)\n",
    "recall_scores = recall_score(ytrue, ypredict, average=None)\n",
    "\n",
    "# Printing recall scores for each class\n",
    "for idx, alphabet in enumerate(alphabets):\n",
    "    print(f\"Recall for Alphabet {alphabet}: {recall_scores[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5bef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(alphabets, recall_scores)\n",
    "plt.xlabel('Alphabets')\n",
    "plt.ylabel('Recall scores')\n",
    "plt.title('Recall score for Each Alphabet')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745c937",
   "metadata": {},
   "source": [
    "##### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b1570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making predictions on train data using trained model\n",
    "# yhat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe227eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytrue = np.argmax(Y_train, axis=1).tolist()\n",
    "# yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efa6be",
   "metadata": {},
   "source": [
    "# 5. Realtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform real-time testing\n",
    "colors = [(255,153,153), (255,178,102), (255, 255, 102), (153, 255,51), (51,153,255), (255,153,204), (255,153,153), (255,178,102), (255, 255, 102), (153, 255,51), (51,153,255), (255,153,204), (255,153,153), (255,178,102), (255, 255, 102), (153, 255,51), (51,153,255), (255,153,204), (255,153,153), (255,178,102), (255, 255, 102), (153, 255,51), (51,153,255), (255,153,204), (255,153,153), (255,178,102)]\n",
    "def prob_viz(res, alphabets, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0, 25+num*25), (int(prob*100), 45+num*25), colors[num], -1)\n",
    "        cv2.putText(output_frame, alphabets[num], (0, 40+num*25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 1, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making real time predictions\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.9\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mediapipe_holistic.Holistic(min_detection_confidence=0.9, min_tracking_confidence=0.9) as holistic:\n",
    "    while cam.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        return_value, image_frame = cam.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, detected_landmarks = detection_function(image_frame, holistic)\n",
    "        print(detected_landmarks)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, detected_landmarks)\n",
    "        \n",
    "        # making Prediction\n",
    "        keypoints = mediapipe_keypoints(detected_landmarks)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(alphabets[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if alphabets[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(alphabets[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(alphabets[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # displaying probability bar\n",
    "            image = prob_viz(res, alphabets, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (1280, 20), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,20), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        # displaying on screen\n",
    "        cv2.imshow('Test model', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, alphabets, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c1928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
